{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML\n",
    "\n",
    "As we discussed in our previous reading, most of the cloud isn't really designed for you, an applied data analyst. Thankfully, Microsoft has created a *really* impressive little world for applied data analysts we'll be using called AzureML. \n",
    "\n",
    "Note that while these services are all labeled as being for \"machine learning\", and they are optimized for standard \"fit, test, predict\"-esque scikit-learn workflows, they are also the place to go for doing other forms of data analysis too.\n",
    "\n",
    "So to get going, we're going to set up the following things. At the moment, we're not going to dive deep into exactly what they all are (because honestly, most have to do with giving people ways to break things up for billing in large organizations and don't matter to an applied data scientist), but we'll circle back and explore them all more later. \n",
    "\n",
    "## Azure Subscription\n",
    "\n",
    "Unsurprisingly, you have to sign up with Azure. The best way to do this will be [go here](https://azure.microsoft.com/en-us/free/students/) and sign up a new account, which will also get you 100 dollars free. Note that if you have already done this and used your free 100 dollars... try and make a new account with a new email address? For example, most universities give you a short-hand email, but you can also get a \"vanity\" long name (e.g. my email is both nce8@duke.edu and nick.eubank@duke.edu). \n",
    "\n",
    "Note that there are ways an organization (like Duke) one can attach your account to a subscription, but in this case I'd like you to get the experience of doing this from step one forward so you know you can do it all on your own!\n",
    "\n",
    "Once you have an account, you should end up at the Azure Portal page:\n",
    "\n",
    "![azure_portal](images/azure_portal.png)\n",
    "\n",
    "\n",
    "## Create a Workspace\n",
    "\n",
    "All work within the AzureML ecosystem happens in a workspace, which you can think of as being like a github repo for your project, able to keep everything associated with a project in one place. \n",
    "\n",
    "So setup a Workspace using the [directions here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace), with a few added notes from me:\n",
    "\n",
    "- You're gonna have to name a LOT of things. Like a crazy number. It's insane how many groups within groups within groups exist in Azure. So for everything I just recommend [your initials]-[name of thing you're naming]. So when you have to name your workspace, I'd call it `nce-ws`. When when I name a resource group, I'd call it `nce-rg`. Later when you're comfortable with Azure, you can get fancy, but for now this will keep you sane. \n",
    "- At the stage it says \"pick a resource group if you have one or create a new name\", you don't have one, so make a new name. See the note above. \n",
    "\n",
    "Once you create a workspace, you'll be brought to a workspace page. Note that as of October 2020, Microsoft was in the process of migrating from one interface to another, so if you see this in the middle of your landing page:\n",
    "\n",
    "![azure_new_mlstudio](images/azure_new_mlstudio.png)\n",
    "\n",
    "Select launch now, and you should end up on a page whose URL starts with `ml.azure.com`, and which looks like this:\n",
    "\n",
    "![azure_ml_studio](images/azure_ml_studio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources & Resource Groups\n",
    "\n",
    "OK, so what was that resource group you created?\n",
    "\n",
    "In Azure, all the concrete things you want to use -- a virtual machine, a distributed cluster, a network storage drive -- are called \"resources\", and every resource has to live in a resource group. I *think* the goal of this is that if all the resources for a project live in one resource group, then when you finish the project you can easily delete them all, and I think they're useful for corporations to manage billing. In this project, we'll only have the one resource group you created when you made your working group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute on Azure\n",
    "\n",
    "In this class, we'll talk about two types of computing resources you can get through AzureML: \n",
    "\n",
    "1. **Rent a single Virtual Machine (VM)**: The first option for compute resources on AzureML is to just rent a single computer with lots of ram and processors. The advantage of this is that it's very easy to setup, you can interact with the VM using Jupyter, Jupyter Lab, or Rstudio's full graphical user interfaces just as you would on your personal laptop, just with more power. The *downside* is that there's a limit to how many resources you can get in a VM, and they tend to be *relatively* expensive (since you're renting premium hardware).\n",
    "2. **Rent a cluster**: Here's the real value of AzureML -- it allows you to very easily setup a *scalable* cluster of as many computers as you want. Generally, you set up your cluster with simple individual computers (\"nodes\") to manage cost (10 basic computers are cheaper than 1 fancy computer), then you use them with a program like dask to do distributed work with large datasets. \n",
    "\n",
    "### Renting a VM\n",
    "\n",
    "To setup compute resources, go into your ML workspace (URL starts with `ml.azure.com`) and click on \"Compute\" towards the bottom of the menu on the left. \n",
    "\n",
    "The first option for adding compute is \"Compute Instances\" -- this is a simple VM. So let's set one up! Click \"Create\", pick \"CPU\" as your virtual machine type for now (you can also pick a GPU-centric VM), then check out all the options for \"Virtual machine sizes\". As you will see, you can get computers with up to 72 individual cores, or up to 256 gb RAM! These are *single machines* with all these resources! Amazing, right? \n",
    "\n",
    "The only catch to be aware of is the cost, which is also in that dropdown -- if you do rent 72 cores, it'll cost you about 3 dollars an hour. If you just have a day of work you really need, this is a great deal; if you plan to be running simulations for a week... well, that's pretty darn expensive (though still way less than buying your own dedicated computer!). \n",
    "\n",
    "To try things out, let's get a basic model -- I'm gonna start a `Standard_D11_v2` for 18 cents an hour. :)\n",
    "\n",
    "Now once you have a VM, you can connect in two ways: you can get your familiar graphical user interface using Remote Desktop, or you can connect via the commandline by enabling SSH (which you will see is an option here). Enabling SSH requires setting a public key, which takes some work, so lets skip it for now and just leave that turned off, but if/when you want that option, you can find directions for setting up [SSH keys here](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/mac-create-ssh-keys#provide-an-ssh-public-key-when-deploying-a-vm). Click \"Create\", and go grab a cup of coffee while your machine gets setup! (You'll have to wait about 5-7 minutes for it to get going). \n",
    "\n",
    "Once it's running, click on the name of your VM in your list of compute resources, and then click on the \"Run\" tab. There, on the right side, you'll see a series of links to Jupyter, JupyterLab, RSTudio, and (if you enabled it) ssh:\n",
    "\n",
    "![azure_vm_services](images/azure_vm_services.png)\n",
    "\n",
    "Just click on those links and the service you requested for the VM will pop up! TA-DA! You're running Jupyter on the cloud!\n",
    "\n",
    "(Because we're in AzureML, the VMs offered are all running the linux operating system and come with standard Data Science software installed. If you want a different kind of VM -- say, a windows machine, or a linux machine without software pre-installed -- go back to your [Azure Portal page](https://portal.azure.com/#home), and select \"Virtual Machines.\" There you can completely control the configuration of your VM, and even set up a remote desktop connection if you want to use a regular Windows experience.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up A Cluster\n",
    "\n",
    "The other thing you can do is setup a full computer cluster, with is a collection of (generally smaller) computers that are all networked together to act as one. \n",
    "\n",
    "The great thing about AzureML is that you can create a cluster that starts with, say, two nodes (computers), but then scale it up to 100 nodes when you want to run a bit of computationally intensive code! Then when you're done, those nodes will automatically shut down when they've been idle for a set period of time, and you'll be back to the two nodes. And that means you only have to pay for the 100 nodes *when you are using them!*. Pretty amazing, honestly. \n",
    "\n",
    "OK, but we're not gonna set up a cluster here in the browser because it's actually *much* easier to set up a cluster from a Python session (especially if you want to use `dask`), so for now I'll just point out that if you click on the \"Compute clusters\" tab of the \"Compute\" section of your workspace, you can see how you set up a cluster from here. It's not hard, but it's even easier from Python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage on Azure\n",
    "\n",
    "As we discussed in the last tutorial, when working on the cloud your \"compute\" resources and your \"storage\" resources are sold (mostly) separately. This isn't entirely true -- your VM and your cluster computers have some harddrive space -- but that memory is tied to that compute, and so when you shut down your compute, anything saved on those harddrives goes away. So normally we need to get some persistent storage for our data and results. \n",
    "\n",
    "Thankfully, when you created your Workspace, Azure also created a \"Storage Account\" for you. Asure not only uses this Storage Account to store all metadata about your Workshop, but automatically connects this Storage Account to whenever you create a VM or cluster!\n",
    "\n",
    "To see this account:\n",
    "\n",
    "1. Go back to your [Azure Portal](portal.azure.com), \n",
    "2. Click on  \"Storage Account.\" \n",
    "\n",
    "There you should see a Storage Account whose name is [your workspace name] plus a long series of random digits. (They add the random numbers because Storage account names are used as URLs, so they have to be unique across all of Azure). \n",
    "\n",
    "While you might *think* that Storage Accounts are where we put our data, though, that's not quite right. Storage Accounts can hold 4 different kinds of things: Blob Containers, File Shares, Tables, and Queues. UGH. \n",
    "\n",
    "The good news is that, at this point in your life, you only need to worry about Blob Containers. As we mentioned in our \"What is the Cloud\" tutorial, Cloud storage comes in different flavors, and Blob is the most flexible, as it can hold anything (BLOB standands for **B**inary **L**arge **OB**ject storage). So unless you have reason to do otherwise, just use Blob Containers. \n",
    "\n",
    "Other types on Azure, just so you know them, are:\n",
    "\n",
    "- **File storage**: similar to Blob. File storage supports certain file sharing protocols that people may have been using before they came to the cloud, so makes life easy for people moving to the cloud. But fewer features than BLOB, so almost surely not for you. *Sounds* like the storage that'd be most familiar to a normal computer user, but it's a trick. Use BLOB. \n",
    "- **Queue storage**: specialized storage for messaging services. \n",
    "- **Table storage**: What's known as a NoSQL database system.\n",
    "\n",
    "And lest this whole \"Storage Account\" / Blob Containers / Files thing is getting confusing (I *know*, so many nested groups!!!), it looks something like this:\n",
    "\n",
    "![azure_blob_containers](images/azure_blob_containers.png)\n",
    "\n",
    "### Create Container and Add Data\n",
    "\n",
    "So let's add some data! On the left menu, and click on \"Containers\" under Blob Services. \n",
    "\n",
    "First, note that there is already a handful of Containers there -- these store the data about your Workspace. \n",
    "\n",
    "But now we can create a new container for our data! So click the \"+ Container\" button, pick a name, and click \"Create\". Congrats! You have a blob!\n",
    "\n",
    "We'll talk more about managing data using Python or the command line later, but for the moment, let's just upload an easy file. Click on your new Container, click \"Upload\" in the top left, and upload a file!\n",
    "\n",
    "### Creating A New Storage Account\n",
    "\n",
    "If at some point you want a NEW Storage Account, just go back to the Portal, click \"Add Resource\" and search for Storage Account. \n",
    "\n",
    "### Managing Storage from your local command line with Azure CLI\n",
    "\n",
    "Since you often want to upload lots of files from your own computer, it's often easiest to do file transfers with the [Azure Command Line Interface (CLI)](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). In addition to creating Blob containers and such, you can also [upload and download files with the CLI](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-cli), or you can use [AzCopy](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10), which can synchronize two folders by seeing how they differ and only transfering changed data back and forth ([azcopy sync](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs#synchronize-files)) kind of like `rsync`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing It All Together: Accessing Data from Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the final move: make your data visible from your virtual machines! We'll do much more of this from within Python in \n",
    "\n",
    "### Mount Your Container as a Drive\n",
    "\n",
    "One way to make your data available is to use `blobfuse` to mount your Blob Container as though it were just a drive on your AzureML machine. `blobfuse` is already installed on all the AzureML machines, so its relatively easy to mount a drive. ([here are the full docs](https://github.com/Azure/azure-storage-fuse)). Just open a terminal (either using SSH, or if you didn't enable SSH, use Jupyter Lab to open a terminal, and run the following after replacing the angle bracket labels with real values (you shouldn't have any angle brackets when you're done!):\n",
    "\n",
    "\n",
    "```\n",
    "# Create a cache on your local SSD\n",
    "sudo mkdir /mnt/blobfusetmp -p\n",
    "sudo chown azureuser /mnt/blobfusetmp\n",
    "\n",
    "# Make the login details visible\n",
    "export AZURE_STORAGE_ACCOUNT=<yourstorageaccountname>\n",
    "export AZURE_STORAGE_ACCESS_KEY=<yourstorageaccountkey>\n",
    "\n",
    "# Create a directory that\n",
    "# will become the mounted drive\n",
    "\n",
    "mkdir <path-to-where-you-want-to-mount>\n",
    "\n",
    "blobfuse <path-to-where-you-want-to-mount> --container-name=<container-to-mount> --tmp-path=/mnt/blobfusetmp -o attr_timeout=240 -o entry_timeout=240 -o negative_timeout=120\n",
    "\n",
    "```\n",
    "\n",
    "And then your Blob container should appear in the folder you made with the `mkdir <path-to-where-you-want-to-mount>` argument. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a Datastore in Workspace\n",
    "\n",
    "**STILL LEARNING THIS PART!** \n",
    "\n",
    "The easy way to make things visible is to \"register\" your Blob Container from within your Workspace as a \"Datastore\". Then you can access it like a mounted drive from inside any of your VMs! \n",
    "\n",
    "To do so, just go to your Workspace, and on the very bottom of the left hand menu click \"Datastores.\" You should see two default Workspace datastores there, but let's make this interesting and add the file we just uploaded above. Now just click \"+ Datastore\" in the top left and connect it to the Blob Container you just created above, and you're done! Note you'll need your Storage Account key, which you can find by going back to the Azure Portal, clicking Storage Accounts, and clicking \"Access Keys\" on the left. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
