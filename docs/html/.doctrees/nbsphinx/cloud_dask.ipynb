{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a Dask Cluster on AzureML\n",
    "\n",
    "In this lesson, we'll be using a dask cluster to replicate the [exercise we did in the Big Data section](exercises/Exercise_bigdata.ipynb) where we loaded global temperature data to measure global warming at a number of locations. You can get the data we're using for this [exercise here](https://www.dropbox.com/s/oq36w90hm9ltgvc/global_climate_data.zip?dl=0)). I'm also assuming you already have a Azure account and an AzureML workspace setup -- [go back here if you don't](cloud_azureml.ipynb)!\n",
    "\n",
    "**Want to use Amazon AWS?** You can do that too! The package we use to launch our cluster below also supports AWS. We won't go through that here, but you can find [info on it here](https://cloudprovider.dask.org/en/stable/index.html).\n",
    "\n",
    "If you want to follow along, just decompress the `ghcnd_daily.tar.gz` file and upload the resulting `.csv` into a Blob container using the web interface. Note this will take a little while. Will talk about more efficient methods of upload in our next tutorial.\n",
    "\n",
    "To run this code, in addition the `dask` and `pandas`, which you should already have installed, you'll need to install the following packages (`azureml-sdk` and `dask_cloudprovider`) with the following commands:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge azure-storage-blob # For managing storage\n",
    "pip install azureml-sdk                         # For managing compute\n",
    "pip install dask_cloudprovider==0.4.1\n",
    "```\n",
    "\n",
    "Note that `dask_cloudprovider` sometimes doesn't load the right version if you don't specify, and as of October 2020 the right version isn't even on `conda-forge`, so don't use `conda install`. You can also pip install `azure-storage-blob` if you prefer `pip` to `conda`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "from azureml.core import Workspace, Experiment\n",
    "from dask_cloudprovider import AzureMLCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm gonna load my subscription_id,\n",
    "# resource_group, and workspace_name from a hidden file\n",
    "# so y'all can't see them! You can just put in your code\n",
    "# if its private.\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"/users/nick/azure_secrets/azure_config.json\") as f:\n",
    "    account = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register workspace with account info\n",
    "ws = Workspace(\n",
    "    subscription_id=account[\"subscription_id\"],\n",
    "    resource_group=account[\"resource_group\"],\n",
    "    workspace_name=account[\"workspace_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start up a cluster!\n",
    "amlcluster = AzureMLCluster(\n",
    "    ws,\n",
    "    vm_size=\"STANDARD_DS3_V2\",  # Azure VM size for the Compute Target,\n",
    "    datastores=ws.datastores.values(),  # Azure ML Datastores to mount on the headnode\n",
    "    environment_definition=ws.environments[\n",
    "        \"AzureML-Dask-CPU\"\n",
    "    ],  # Azure ML Environment to run on the cluster\n",
    "    jupyter=True,  # Flag to start JupyterLab session on the headnode\n",
    "    initial_node_count=4,  # number of nodes to start\n",
    "    scheduler_idle_timeout=7200,  # scheduler idle timeout in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Your Cluster\n",
    "\n",
    "There are two ways to use your cluster: You can click on the link above to open a connection to JupyterLab running on one of the computers in your cluster, or connect from here with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "c = Client(amlcluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're off to the races! One note though -- if you decide to work from your own computer, you may get a warning about version differences between `dask` on the cloud on and on your own computer. I initially got:\n",
    "\n",
    "```\n",
    "/Users/Nick/miniconda3/lib/python3.7/site-packages/distributed/client.py:1130: VersionMismatchWarning: Mismatched versions found\n",
    "\n",
    "+---------+---------------+---------------+---------------+\n",
    "| Package | client        | scheduler     | workers       |\n",
    "+---------+---------------+---------------+---------------+\n",
    "| lz4     | None          | 3.1.0         | 3.1.0         |\n",
    "| numpy   | 1.19.1        | 1.19.2        | 1.19.2        |\n",
    "| python  | 3.7.8.final.0 | 3.6.9.final.0 | 3.6.9.final.0 |\n",
    "+---------+---------------+---------------+---------------+\n",
    "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n",
    "```\n",
    "\n",
    "The `numpy` and `python` issues don't seem like to cause big problems (though if you have a mismatch and get problems down the road, consider changing your Python version!), but the fact that the schedulers and workers have one package I don't (`lz4`) is a problem, so I installed it before moving forward. (`lz4` is a compression algorithm used to send data back and forth, so not having it is a big problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll contect to the container where I put the climate data. As you saw in the last exercise, you can upload data using the Azure web interface, and I would do that if you want to do these exercises. However there are more efficient tools we'll cover in the [next lesson](cloud_azurestorage.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Load connection string so y'all can't see it!\n",
    "with open(\"/users/nick/azure_secrets/azure_sa_connection_string.txt\") as f:\n",
    "    connection_string = f.read()\n",
    "\n",
    "# Connect to storage account\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container = blob_service_client.get_container_client(\"globaltemps\")\n",
    "\n",
    "# Look at the files in the container for sanity check.\n",
    "for f in container.list_blobs():\n",
    "    print(f[\"name\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step above was a fun way to see what's in my folders, but it's not actually required for using `dask` because `dask` can access Azure storage without the help of any other libraries -- you just need to be able to pass it your Storage Account name and Access Key, which you can find by going to your Azure Portal, then your Storage Account, and then clicking on \"Access Keys\" on the left menu. The syntax is:\n",
    "\n",
    "```\n",
    "import dask.dataframe as dd\n",
    "\n",
    "storage_options={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}\n",
    "\n",
    "ddf = dd.read_csv('az://{CONTAINER}/{FOLDER}/*.csv', storage_options=storage_options)\n",
    "ddf = dd.read_parquet('az://{CONTAINER}/folder.parquet', storage_options=storage_options)\n",
    "```\n",
    "\n",
    "But since I don't want you to see all my secret codes, I'm gonna load my information from a file. You can do this, but you can also put them in your code *if your code isn't public!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask connects with a protocl\n",
    "import json\n",
    "\n",
    "with open(\"/users/nick/azure_secrets/azure_sa_name_and_key.json\") as f:\n",
    "    storage_options = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# dask things the flag columns are floats, when really objects.\n",
    "flag_dict = {}\n",
    "for i in range(1, 32):\n",
    "    for flag in [\"q\", \"m\"]:\n",
    "        flag_dict.update({f\"{flag}flag{i}\": \"object\"})\n",
    "\n",
    "temps = dd.read_csv(\n",
    "    f\"az://globaltemps/ghcnd_daily.csv\",\n",
    "    storage_options=storage_options,\n",
    "    dtype=flag_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that I added some code to explicitly name the types of some columns (the flags). As you may recall from our other lesson, dask isn't great at type inference, and will otherwise assume those are `float` columns instead of `objects`. Without that code, I get this error:\n",
    "\n",
    "```\n",
    "/azureml-envs/azureml_c6bd17107f471892400d23146f291775/lib/python3.6/site-packages/dask/dataframe/io/csv.py in coerce_dtypes()\n",
    "\n",
    "ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n",
    "\n",
    "+---------+--------+----------+\n",
    "| Column  | Found  | Expected |\n",
    "+---------+--------+----------+\n",
    "| qflag1  | object | float64  |\n",
    "| qflag10 | object | float64  |\n",
    "| qflag11 | object | float64  |\n",
    "| qflag12 | object | float64  |\n",
    "| qflag13 | object | float64  |\n",
    "| qflag14 | object | float64  |\n",
    "| qflag15 | object | float64  |\n",
    "| qflag16 | object | float64  |\n",
    "| qflag17 | object | float64  |\n",
    "| qflag18 | object | float64  |\n",
    "| qflag19 | object | float64  |\n",
    "| qflag2  | object | float64  |\n",
    "| qflag20 | object | float64  |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by asking `dask` to go through this entire dataset and just pull out data for the station near my home in Colorado, and calculating the average daily max-temp for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = temps[temps[\"id\"] == \"USC00050848\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you may recall, `dask` hasn't actually run the code above -- it's just making a plan and waiting till I run `.compute()` to actually execute, so if I check on `temps`, I'll see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we'll most want to work with this data from one station, we can cache this subset dataframe with `c.persist(temps)` so `dask` won't have to re-load the original data next time we run `.compute()`. [More on caching here](https://docs.dask.org/en/latest/dataframe-best-practices.html#persist-intelligently), as well as other [dask best practices](https://docs.dask.org/en/latest/dataframe-best-practices.html#best-practices). Make sure to not just run `c.persists(temps)` but rather `temps = c.persist(temps)`.\n",
    "\n",
    "Finally, one other note: this dataset is actually pretty small for Cloud computing, and since most of what we're doing is reading in data and filtering it, it involves a lot of moving data around. As a result it'd be much faster on a single really large VM (which doesn't need to use network connections to pass around data). But I wanted an example I could run relatively quickly and easily. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = c.persist(temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we have our subset, and we can calculate what we want. Note that in reality, the data for this one station is actually easily small enough to put on my own computer, so I could have just run `data_on_own_comp = temps.compute()` above and moved the final dataset to my personal computer. Indeed, as you may recall from our [previous lesson on parallelism](parallelism.ipynb), if you don't have to use distributed computing, you probably don't want to! Even the authors of `dask` [are quick to remind users](https://docs.dask.org/en/latest/dataframe-best-practices.html#use-pandas): \"For data that fits into RAM, Pandas can often be faster and easier to use than Dask DataFrame. While 'Big Data' tools can be exciting, they are almost always worse than normal data tools while those remain appropriate.\"\n",
    "\n",
    "But let's carry on with this on the cluster for practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps[\"value31\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the really cool thing: if we use the link for our dashboard we got when we created our dask cluster, we can see our cluster working:\n",
    "\n",
    "![azure_dask_scalingup](images/azure_dask_scalingup.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's replace those with missing, calculate averages across all the days in each month, and also convert from 1/10th of a degree Centigrade to Centigrade units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "for i in range(1, 32):\n",
    "    temps[f\"value{i}\"] = temps[f\"value{i}\"].replace(-9999, np.nan)\n",
    "\n",
    "value_columns = [i for i in temps.columns if re.match(\"value.*\", \"value.*\")]\n",
    "temps[\"avg\"] = temps[value_columns].mean(axis=\"columns\")\n",
    "temps[\"avg_C\"] = temps[\"avg\"] / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally we can collect our results on our personal computer, create a \"time\" variable that uses years and months for plotting, and plot our temperatures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps[\"time\"] = temps[\"year\"] + (temps[\"month\"] - 1) / 12\n",
    "temps = temps[[\"time\", \"avg_C\"]]\n",
    "df = temps.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "(\n",
    "    ggplot(df, aes(x=\"time\", y=\"avg_C\"))\n",
    "    + geom_line()\n",
    "    + geom_smooth(method=\"lowess\", color=\"red\")\n",
    "    + ylab(\"Monthly Average of Daily Highs (C)\")\n",
    "    + xlab(\"Year\")\n",
    "    + ggtitle(\"Temperatures in Boulder, CO\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that, my friends, is global warming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done with your cluster?\n",
    "\n",
    "You have two options: you can shut it down manually with the method `.close()` on your cluster object. Thankfully, though, we also set a timeout condition when we created our cluster with the `scheduler_idle_timeout` keyword. We set it to 7200, so if your cluster is idle for two-hours, it will shut itself down (so you don't go broke). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlcluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For some reason when I get this I get the error above, but I can confirm on the Azure cite my cluster has been deleted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions and Fun Resources\n",
    "\n",
    "- `dask-ml`: As a reminder, if you now want to do some machine learning, you can [use dask-ml on this system](https://ml.dask.org/), which does the same thing for `scikit-learn` that regular `dask` does for `pandas`. \n",
    "- Parallelize your own code with `delayed`: Just a reminder that while we've been using `dask` to emulate pandas in a distributed setting, it's also a framework you can use for distributing your own code! [Check out the `delayed`](https://docs.dask.org/en/latest/delayed.html) method to see how `dask` can manage your distributed workload. \n",
    "- Curious how `dask` compares to other tools for distributed computing? Here's a [conceptual comparison to Spark](https://docs.dask.org/en/latest/spark.html), and here's a [case study comparison of performance](https://arxiv.org/abs/1907.13030). Comparisons will usually depend a lot on the specifics of the work being done, but at least in this case, `dask` was a little faster than Spark. In the interview noted below, they also cite an example of `dask` beating Spark by 40x in a project they worked on. And here's [one report of a 2000x speed up doing random forests](https://www.saturncloud.io/s/random-forest-on-gpus-2000x-faster-than-apache-spark/) moving from Spark to dask. As they say, mileage may vary, but I think it's safe to say you aren't giving anything up performance wise by using `dask` and its familiar syntax instead of Spark.\n",
    "- Interested in using `dask` in your company and want help? There's a great new company created by the founders of `dask` to provide enterprise support for `dask` [called coiled](https://coiled.io/) (No, I have no affiliation with them, I just think these companies that try to offer paid support services to businesses to help them move from closed source software to open source are a great way to help make open source software better). You can also hear a fun interview with the founders about [both `dask` and coiled here](https://talkpython.fm/episodes/show/285/dask-as-a-platform-service-with-coiled).\n",
    "- The folks from coiled have also compiled a [great collection of videos and tutorials about dask and Python at scale here](https://coiled.io/videos/)\n",
    "- Working with GPUs? There's a project to offer the kind of CPU parallelization we get from `dask` for GPUs called [dask-cudf](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html) (part of the [RAPIDS project](https://rapids.ai/index.html). The project is young but growing quickly. My guess, though, is that those libraries will become the infrastructure for updates to tools like `dask-ml` rather than something most applied people need to play with. But putting it here as an FYI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "OK, that's our short demo of computing on Azure with dask. I hope you found it useful!\n",
    "\n",
    "As for next steps, I'd suggest the following readings, after which you should go play with your own project!\n",
    "\n",
    "- [Managing Storage](cloud_azurestorage.ipynb): A discussion of better tools for uploading, downloading, and synchronizing data.\n",
    "- [Mounting Storage](cloud_mountingazure.ipynb): A way to set-up easier access to your storage from your compute nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
